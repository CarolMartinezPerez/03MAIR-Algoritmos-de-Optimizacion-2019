{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.10"
    },
    "colab": {
      "name": "Introducción al aprendizaje por refuerzo - Ejercicio de evaluación.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CarolMartinezPerez/03MAIR-Algoritmos-de-Optimizacion-2019/blob/master/RL/Introduccio%CC%81n_al_aprendizaje_por_refuerzo_Ejercicio_de_evaluacio%CC%81n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvol8eULk_af",
        "colab_type": "text"
      },
      "source": [
        "### Teoría"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtzOYaKXk_ao",
        "colab_type": "text"
      },
      "source": [
        "- Esta parte será el 40% de la nota final del bloque de Aprendizaje por Refuerzo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul6plsBNk_aq",
        "colab_type": "text"
      },
      "source": [
        "Define brevemente qué es el aprendizaje por refuerzo. ¿Qué diferencias hay entre aprendizaje supervisado, no supervisado y por refuerzo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXzPaFP_k_as",
        "colab_type": "text"
      },
      "source": [
        "La diferencia es principalmente que el **Aprendizaje No supervisado**, a diferencia de los otros, no tiene información real del problema, ofrece una solución de identificación de patrones y procesos. En el **Aprendizaje Supervisado** la salida es conocida, y se entrena al sistema para enfrentarse a nuevas entradas. En el **Aprendizaje por Refuerzo** la respuesta no aparece normalmente como única, o al menos no el conjunto de respuestas, sino que trata de escoger la idónea según una valoración obtenida para llegar a un determinado objetivo global.\n",
        "\n",
        "\n",
        "\n",
        "Según lo comentado en clase se podría tener en cuenta también como la siguiente diferenciación:\n",
        "\n",
        " - A. No supervisado -> descriptivo\n",
        " - A. Supervisado -> productivo\n",
        " - A. por refuerzo -> prescriptivo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIl1Qorok_at",
        "colab_type": "text"
      },
      "source": [
        "Define con tus palabras los conceptos de Entorno, Agente, Recompensa, Estado y Observación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGIgRQQSk_av",
        "colab_type": "text"
      },
      "source": [
        "- **Entorno**: Contexto en el que el agente debe escoger una respuesta o acción\n",
        "- **Agente**: Ente que analiza la información e intenta escoger la mejor respuesta o acción\n",
        "- **Recompensa**: Valoración a una determinada acción o respuesta\n",
        "- **Estado**: Momento de decisión del agente sobre qué acción o respuesta dar para pasar al siguiente estado\n",
        "- **Observación**: Valor de cada parámetro que constituye el entorno en un instante dado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9famgakok_ax",
        "colab_type": "text"
      },
      "source": [
        "Dependiendo del algoritmo de aprendizaje por refuerzo que se use, ¿qué clasificaciones podemos encontrar? Coméntalas brevemente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za_3PRJTk_ay",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QteRBe08k_az",
        "colab_type": "text"
      },
      "source": [
        "Lista tres diferencias entre los algoritmos de DQN y Policy Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8o5isCKk_a1",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A15IoVvHk_a2",
        "colab_type": "text"
      },
      "source": [
        "### Práctica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPwsnniIk_a3",
        "colab_type": "text"
      },
      "source": [
        "- Esta parte será el 60% de la nota final del bloque de Aprendizaje por Refuerzo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR_OsU4Xk_a4",
        "colab_type": "text"
      },
      "source": [
        "Algunas consideraciones a tener en cuenta:\n",
        "\n",
        "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "- Para nuestro ejercicio, una solución óptima será alcanzar una media de recompensa por encima de 16 puntos. Para medir si hemos conseguido llegar a la solución óptima, la media de la recompensa se calculará a partir del código de test en la última celda del notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHO6ZiC_k_a6",
        "colab_type": "text"
      },
      "source": [
        "Este bloque práctico consta de tres partes:\n",
        "\n",
        "   1) Implementar la red neuronal que se usará en la solución\n",
        "    \n",
        "   2) Seleccionar los hiperparámetros adecuados para las distintas piezas de la solución DQN\n",
        "    \n",
        "   3) Justificar la respuesta en relación a los resultados obtenidos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGNOZpK8k_a7",
        "colab_type": "text"
      },
      "source": [
        "IMPORTANTE:\n",
        "\n",
        "- Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
        "\n",
        "- Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
        "\n",
        "- Si usáis Google Colab, recordad usar las versiones de Tensorflow==1.13.1, Keras==2.2.4 y keras-rl==0.4.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWujNMZkk_a8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment this line for installing keras-rl on Google collaboratory\n",
        "# !pip install keras-rl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhFqw8_uk_bK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LnN626Uk_bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "\n",
        "env_name = 'SpaceInvaders-v0'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKpoHBxHk_be",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiM3iL21k_bw",
        "colab_type": "text"
      },
      "source": [
        "1) Implementación de la red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG4s69Jgk_bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "model = Sequential()\n",
        "model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
        "\n",
        "# TODO\n",
        "\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZf7KTqVk_b5",
        "colab_type": "text"
      },
      "source": [
        "2) Selección de hiperparámetros para la solución DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwU67rdok_b6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO - Select the parameters for the memory\n",
        "memory = SequentialMemory()\n",
        "processor = AtariProcessor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P4P8wSOk_cE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO - Select the parameters for the policy\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4llt0GGk_cP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO - Select the parameters for the Agent and the Optimizer\n",
        "dqn = DQNAgent()\n",
        "dqn.compile(Adam(), metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jGBTlZqk_cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training part\n",
        "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
        "checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
        "log_filename = 'dqn_{}_log.json'.format(env_name)\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]\n",
        "\n",
        "# TODO - Select the parameters for the method \"fit\"\n",
        "dqn.fit()\n",
        "\n",
        "dqn.save_weights(weights_filename, overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25j1q5VDk_c2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing part to calculate the mean reward\n",
        "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
        "dqn.load_weights(weights_filename)\n",
        "dqn.test(env, nb_episodes=10, visualize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9eQHfpUk_c8",
        "colab_type": "text"
      },
      "source": [
        "3) Justificación de los parámetros seleccionados y de los resultados obtenidos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g9nsmzfk_c9",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}